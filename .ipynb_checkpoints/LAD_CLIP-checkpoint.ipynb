{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import LambdaLR, StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "\n",
    "import gzip\n",
    "import html\n",
    "import os\n",
    "from functools import lru_cache\n",
    "\n",
    "import ftfy\n",
    "import regex as re\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "    The reversible bpe codes work on unicode strings.\n",
    "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "    \"\"\"\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def basic_clean(text):\n",
    "    text = ftfy.fix_text(text)\n",
    "    text = html.unescape(html.unescape(text))\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def whitespace_clean(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "class SimpleTokenizer(object):\n",
    "    def __init__(self, bpe_path: str = \"clip/bpe_simple_vocab_16e6.txt.gz\"):\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
    "        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split('\\n')\n",
    "        merges = merges[1:49152-256-2+1]\n",
    "        merges = [tuple(merge.split()) for merge in merges]\n",
    "        vocab = list(bytes_to_unicode().values())\n",
    "        vocab = vocab + [v+'</w>' for v in vocab]\n",
    "        for merge in merges:\n",
    "            vocab.append(''.join(merge))\n",
    "        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n",
    "        self.encoder = dict(zip(vocab, range(len(vocab))))\n",
    "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
    "        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n",
    "        self.pat = re.compile(r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token+'</w>'\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = ' '.join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_tokens = []\n",
    "        text = whitespace_clean(basic_clean(text)).lower()\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = ''.join([self.decoder[token] for token in tokens])\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 151,277,313\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "# from clip import clip\n",
    "# from clip import model\n",
    "\n",
    "# model, preprocess = clip.load(\"ViT-B/32\", device='cuda', jit=False)\n",
    "\n",
    "model = torch.jit.load(\"../checkpoints/model.pt\").cuda().eval()\n",
    "input_resolution = model.input_resolution.item()\n",
    "context_length = model.context_length.item()\n",
    "vocab_size = model.vocab_size.item()\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load LAD Dataset\n",
    "## Option 1 Load Dataset from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_root = '/media/hxd/82231ee6-d2b3-4b78-b3b4-69033720d8a8/MyDatasets/LAD'\n",
    "data_root = file_root + '/LAD_annotations/'\n",
    "img_root = file_root + '/LAD_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load attributes list\n",
    "attributes_list_path = data_root + 'attribute_list.txt'\n",
    "fsplit = open(attributes_list_path, 'r', encoding='UTF-8')\n",
    "lines_attribute = fsplit.readlines()\n",
    "fsplit.close()\n",
    "list_attribute = list()\n",
    "list_attribute_value = list()\n",
    "for each in lines_attribute:\n",
    "    tokens = each.split(', ')\n",
    "    list_attribute.append(tokens[0])\n",
    "    list_attribute_value.append(tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load label list\n",
    "label_list_path = data_root + 'label_list.txt'\n",
    "fsplit = open(label_list_path, 'r', encoding='UTF-8')\n",
    "lines_label = fsplit.readlines()\n",
    "fsplit.close()\n",
    "list_label = dict()\n",
    "list_label_value = list()\n",
    "for each in lines_label:\n",
    "    tokens = each.split(', ')\n",
    "    list_label[tokens[0]]=tokens[1]\n",
    "    list_label_value.append(tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hxd/anaconda3/envs/py38/lib/python3.8/site-packages/torchvision/transforms/transforms.py:280: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "\n",
    "preprocess = Compose([\n",
    "    Resize((224, 224), interpolation=Image.BICUBIC),\n",
    "    CenterCrop((224, 224)),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "image_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).cuda()\n",
    "image_std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the labels, attributes, images data from the LAD dataset\n",
    "attributes_per_class_path = data_root + 'attributes.txt'\n",
    "fattr = open(attributes_per_class_path, 'r', encoding='UTF-8')\n",
    "lines_attr = fattr.readlines()\n",
    "fattr.close()\n",
    "images = list()\n",
    "attr = list()\n",
    "labels = list()\n",
    "for each in lines_attr:\n",
    "    tokens = each.split(', ')\n",
    "    labels.append(list_label[tokens[0]])\n",
    "    img_path = tokens[1]\n",
    "    image = preprocess(Image.open(os.path.join(img_root, img_path)).convert(\"RGB\"))\n",
    "    images.append(image)\n",
    "    attr_r = list(map(int, tokens[2].split()[1:-1]))\n",
    "    attr.append([val for i,val in enumerate(list_attribute_value) if attr_r[i] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump processed image and text to local\n",
    "with open('../checkpoints/data_img_raw.pkl', 'wb') as file:\n",
    "    pickle.dump(images, file)\n",
    "with open('../checkpoints/data_txt_raw.pkl', 'wb') as file:\n",
    "    pickle.dump({'label': labels, 'att': attr}, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2 Load LAD Dataset from Saved Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../checkpoints/data_img_raw.pkl', 'rb') as file:\n",
    "    images = pickle.load(file)\n",
    "with open('../checkpoints/data_txt_raw.pkl', 'rb') as file:\n",
    "    b = pickle.load(file)\n",
    "    \n",
    "labels = b['label']\n",
    "attr = b['att']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Obtain the Image and Text Features\n",
    "## Option 1 Load CLIP to obtain features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize images\n",
    "image_input = torch.tensor(np.stack(images)).cuda()\n",
    "image_input -= image_mean[:, None, None]\n",
    "image_input /= image_std[:, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to tokens\n",
    "tokenizer_label = SimpleTokenizer()\n",
    "text_tokens = [tokenizer_label.encode(desc) for desc in labels]\n",
    "\n",
    "sot_token = tokenizer_label.encoder['<|startoftext|>']\n",
    "eot_token = tokenizer_label.encoder['<|endoftext|>']\n",
    "\n",
    "text_inputs_label = torch.zeros(len(text_tokens), model.context_length, dtype=torch.long)\n",
    "for i, tokens in enumerate(text_tokens):\n",
    "    tokens = [sot_token] + tokens + [eot_token]\n",
    "    text_inputs_label[i, :len(tokens)] = torch.tensor(tokens)\n",
    "text_inputs_label = text_inputs_label.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert attributes to tokens\n",
    "tokenizer_att = SimpleTokenizer()\n",
    "text_tokens = [[tokenizer_att.encode(desc) for desc in att] for att in attr]\n",
    "\n",
    "sot_token = tokenizer_att.encoder['<|startoftext|>']\n",
    "eot_token = tokenizer_att.encoder['<|endoftext|>']\n",
    "text_inputs_att = list()\n",
    "\n",
    "for j, tokens_img in enumerate(text_tokens):\n",
    "    text_input = torch.zeros(len(tokens_img), model.context_length, dtype=torch.long)\n",
    "    for i, tokens in enumerate(tokens_img):\n",
    "        tokens = [sot_token] + tokens + [eot_token]\n",
    "        text_input[i, :len(tokens)] = torch.tensor(tokens)\n",
    "    text_inputs_att.append(text_input.cuda())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1849104/3267874097.py:2: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448278899/work/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  image_features = model.encode_image(image_input).float()\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    label_fea = model.encode_text(text_inputs_label.cuda()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    text_feature = list()\n",
    "    for txt in text_inputs_att:\n",
    "        if len(txt) == 0:\n",
    "            text_feature.append(torch.empty(0, 512).cuda())\n",
    "        else:\n",
    "            text_feature.append(model.encode_text(txt).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "label_fea /= label_fea.norm(dim=-1, keepdim=True)\n",
    "\n",
    "text_feature = torch.stack([torch.mean(item,0) for item in text_feature])\n",
    "text_feature /= text_feature.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save image and text features\n",
    "with open('../checkpoints/data_txt_feature.pkl', 'wb') as file:\n",
    "    pickle.dump({'label': label_fea, 'att': text_feature}, file)\n",
    "with open('../checkpoints/data_img_feature.pkl', 'wb') as file:\n",
    "    pickle.dump(image_features, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 2 Load saved image and text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../checkpoints/data_txt_feature.pkl', 'rb') as file:\n",
    "    b = pickle.load(file)\n",
    "\n",
    "label_fea = b['label']\n",
    "text_feature = b['att']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../checkpoints/data_img_feature.pkl', 'rb') as file:\n",
    "    image_features = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the dataloader for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from sklearn import preprocessing\n",
    "\n",
    "class Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, image_features, text_feature, labels, data_indx):\n",
    "        self.image_features = image_features\n",
    "        self.text_feature = text_feature\n",
    "        self.labels = labels\n",
    "        self.data_indx = data_indx\n",
    "#         self.imgs = image_input\n",
    "#         self.attr = attr\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample = {'image': self.image_features[idx], \n",
    "                  'attribute': self.text_feature[idx], \n",
    "                  'label': self.labels[idx],\n",
    "                  'data_indx': self.data_indx[idx]\n",
    "#                   'imgs': self.imgs[idx],\n",
    "#                   'attr': self.attr[idx]\n",
    "                 }\n",
    "\n",
    "        return sample\n",
    "   \n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "class_list = list(le.classes_)\n",
    "labels_list = torch.tensor(le.transform(labels)).cuda()\n",
    "\n",
    "attr_ = [';'.join(attr[0]) for item in attr]\n",
    "data_indx = list(range(4600))\n",
    "# dataset = Dataset(image_features, text_feature, labels_list, torch.tensor(np.stack(images)).cuda(), attr_)\n",
    "dataset = Dataset(image_features, text_feature, labels_list, data_indx)\n",
    "train_set, test_set = torch.utils.data.random_split(dataset,[4600-500,500])\n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "# defining the model architecture\n",
    "class Net(nn.Module):   \n",
    "  def __init__(self):\n",
    "      super(Net, self).__init__()\n",
    "\n",
    "      self.linear_layers = nn.Sequential(\n",
    "          nn.Linear(1024, 512),\n",
    "          nn.Linear(512, 230)\n",
    "      )\n",
    "\n",
    "  # Defining the forward pass    \n",
    "  def forward(self, x, t):\n",
    "      con = torch.cat((x, t), 1)\n",
    "      out = self.linear_layers(con)\n",
    "      return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().cuda()\n",
    "error = nn.CrossEntropyLoss().cuda()\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train_loss: 0.047759719650919845, train_accuracy: 0.2975609756097561%, test_loss: 0.0199383544921875, test_accuracy: 0.626%\n",
      "Epoch: 1, train_loss: 0.010961451014367546, train_accuracy: 0.7914634146341464%, test_loss: 0.011954001426696777, test_accuracy: 0.792%\n",
      "Epoch: 2, train_loss: 0.005725246373473145, train_accuracy: 0.8939024390243903%, test_loss: 0.0018081858158111573, test_accuracy: 0.792%\n",
      "Epoch: 3, train_loss: 0.002696266144616302, train_accuracy: 0.9490243902439024%, test_loss: 6.169255450367928e-05, test_accuracy: 0.858%\n",
      "Epoch: 4, train_loss: 0.0013520190019796535, train_accuracy: 0.9782926829268292%, test_loss: 0.0012208878993988036, test_accuracy: 0.864%\n",
      "Epoch: 5, train_loss: 0.0012840903841140794, train_accuracy: 0.9819512195121951%, test_loss: 0.0001842034310102463, test_accuracy: 0.864%\n",
      "Epoch: 6, train_loss: 0.0005731607787311077, train_accuracy: 0.9917073170731707%, test_loss: 0.0013498475551605225, test_accuracy: 0.884%\n",
      "Epoch: 7, train_loss: 0.0008861863283758483, train_accuracy: 0.9826829268292683%, test_loss: 0.00024289563298225404, test_accuracy: 0.886%\n",
      "Epoch: 8, train_loss: 0.0005998159925703232, train_accuracy: 0.9902439024390244%, test_loss: 0.00013991950452327728, test_accuracy: 0.89%\n",
      "Epoch: 9, train_loss: 0.00035051648118873923, train_accuracy: 0.9946341463414634%, test_loss: 0.00011046461015939712, test_accuracy: 0.89%\n",
      "Epoch: 10, train_loss: 0.00011639534245903899, train_accuracy: 0.9985365853658537%, test_loss: 1.2775518000125885e-05, test_accuracy: 0.896%\n",
      "Epoch: 11, train_loss: 6.499487747688119e-05, train_accuracy: 1.0%, test_loss: 5.4937940090894696e-05, test_accuracy: 0.898%\n",
      "Epoch: 12, train_loss: 6.362593673146898e-05, train_accuracy: 1.0%, test_loss: 0.00034964525699615476, test_accuracy: 0.898%\n",
      "Epoch: 13, train_loss: 5.620986578145587e-05, train_accuracy: 1.0%, test_loss: 5.933334678411484e-05, test_accuracy: 0.898%\n",
      "Epoch: 14, train_loss: 5.2860194033511526e-05, train_accuracy: 1.0%, test_loss: 5.308554321527481e-05, test_accuracy: 0.896%\n",
      "Epoch: 15, train_loss: 5.039879795243373e-05, train_accuracy: 1.0%, test_loss: 3.925072960555554e-06, test_accuracy: 0.894%\n",
      "Epoch: 16, train_loss: 5.108877354892107e-05, train_accuracy: 1.0%, test_loss: 0.00019007441401481628, test_accuracy: 0.894%\n",
      "Epoch: 17, train_loss: 4.7076652536350415e-05, train_accuracy: 1.0%, test_loss: 1.7780134454369546e-05, test_accuracy: 0.888%\n",
      "Epoch: 18, train_loss: 4.621231008502768e-05, train_accuracy: 1.0%, test_loss: 4.428032040596008e-05, test_accuracy: 0.89%\n",
      "Epoch: 19, train_loss: 4.4482753260015715e-05, train_accuracy: 1.0%, test_loss: 3.4897830337286e-05, test_accuracy: 0.89%\n",
      "Epoch: 20, train_loss: 4.28462291999561e-05, train_accuracy: 1.0%, test_loss: 3.27441468834877e-05, test_accuracy: 0.89%\n",
      "Epoch: 21, train_loss: 4.370108607406841e-05, train_accuracy: 1.0%, test_loss: 0.00010193941742181777, test_accuracy: 0.89%\n",
      "Epoch: 22, train_loss: 4.2217147022644736e-05, train_accuracy: 1.0%, test_loss: 7.306153886020183e-06, test_accuracy: 0.89%\n",
      "Epoch: 23, train_loss: 4.2204602860005167e-05, train_accuracy: 1.0%, test_loss: 1.5095522627234458e-05, test_accuracy: 0.89%\n",
      "Epoch: 24, train_loss: 4.3904819616639036e-05, train_accuracy: 1.0%, test_loss: 0.00014409585297107697, test_accuracy: 0.89%\n",
      "Epoch: 25, train_loss: 4.1854783392897464e-05, train_accuracy: 1.0%, test_loss: 9.19707864522934e-06, test_accuracy: 0.892%\n",
      "Epoch: 26, train_loss: 4.1993607358052966e-05, train_accuracy: 1.0%, test_loss: 2.6926299557089806e-05, test_accuracy: 0.892%\n",
      "Epoch: 27, train_loss: 4.275702593121223e-05, train_accuracy: 1.0%, test_loss: 9.08774584531784e-05, test_accuracy: 0.892%\n",
      "Epoch: 28, train_loss: 4.498129012063146e-05, train_accuracy: 1.0%, test_loss: 0.0002562724649906158, test_accuracy: 0.892%\n",
      "Epoch: 29, train_loss: 4.163291356412739e-05, train_accuracy: 1.0%, test_loss: 3.178300708532333e-05, test_accuracy: 0.892%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "# Lists for visualization of loss and accuracy \n",
    "epoch_list = []\n",
    "train_accuracy_list = []\n",
    "train_loss_list = []\n",
    "valid_accuracy_list = []\n",
    "valid_loss_list = []\n",
    "PATH = \"../checkpoints/cnn.pth\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    correct = 0\n",
    "    running_loss = 0\n",
    "    model.train()\n",
    "    for data in trainloader:\n",
    "        # Transfering images and labels to GPU if available\n",
    "#         image_batch, text_batch, label_batch, im_batch, att_batch = data['image'], data['attribute'], data['label'], data['imgs'], data['attr']\n",
    "        image_batch, text_batch, label_batch, idx_batch = data['image'], data['attribute'], data['label'], data['data_indx']\n",
    "        # Forward pass \n",
    "        outputs = model(image_batch, text_batch)\n",
    "        #CrossEntropyLoss expects floating point inputs and long labels.\n",
    "        loss = error(outputs, label_batch)\n",
    "        # Initializing a gradient as 0 so there is no mixing of gradient among the batches\n",
    "        optimizer.zero_grad()\n",
    "        #Propagating the error backward\n",
    "        loss.backward()\n",
    "        # Optimizing the parameters\n",
    "        optimizer.step()\n",
    "    \n",
    "        predictions = torch.max(outputs, 1)[1].cuda()\n",
    "        correct += (predictions == label_batch).sum()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_loss_list.append(float(running_loss) / float(len(trainloader.dataset)))\n",
    "    train_accuracy_list.append(float(correct) / float(len(trainloader.dataset)))\n",
    "    \n",
    "    # test on validation set\n",
    "    correct = 0\n",
    "    running_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            image_batch, text_batch, label_batch, idx_batch = data['image'], data['attribute'], data['label'], data['data_indx']\n",
    "\n",
    "            outputs = model(image_batch, text_batch)\n",
    "\n",
    "            predictions = torch.max(outputs, 1)[1].cuda()\n",
    "            correct += (predictions == label_batch).sum()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "                               \n",
    "    valid_loss_list.append(float(running_loss) / float(len(testloader.dataset)))\n",
    "    valid_accuracy_list.append(float(correct) / float(len(testloader.dataset)))\n",
    "                               \n",
    "    print(\"Epoch: {}, train_loss: {}, train_accuracy: {}%, test_loss: {}, test_accuracy: {}%\".format(epoch, \n",
    "                                                      train_loss_list[-1], \n",
    "                                                      train_accuracy_list[-1], \n",
    "                                                      valid_loss_list[-1], \n",
    "                                                      valid_accuracy_list[-1]))\n",
    "            \n",
    "    \n",
    "                          \n",
    "    epoch_list.append(epoch)     \n",
    "    scheduler.step()\n",
    "    \n",
    "    if (epoch % 10) == 0:\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(testloader))\n",
    "image_batch, text_batch, label_batch, idx_batch = data['image'], data['attribute'], data['label'], data['data_indx']\n",
    "outputs = model(image_batch, text_batch)\n",
    "\n",
    "for id in range(64):\n",
    "    plt.imshow(images[idx_batch[id]].cpu().detach().permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    print(m(outputs[id]).cpu().topk(3, dim=-1))\n",
    "    top3 = m(outputs[id]).cpu().topk(3, dim=-1).indices\n",
    "    print([class_list[i] for i in top3])\n",
    "    print(attr[idx_batch[id]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
